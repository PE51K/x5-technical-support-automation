# X5 Technical Support Automation - Refactored Architecture

This directory contains the refactored X5 Technical Support Automation system with a clean separation between FastAPI backend and Gradio frontend.

## ğŸ—ï¸ Architecture Overview

```
src/
â”œâ”€â”€ ai/                          # AI features and workflow logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ workflow.py              # Main AssistantFlow workflow
â”‚   â”œâ”€â”€ workflow_with_tracing.py # Workflow with Langfuse tracing
â”‚   â”œâ”€â”€ workflow_events.py       # Workflow event definitions
â”‚   â””â”€â”€ workflow_steps/          # Individual workflow steps
â”œâ”€â”€ api/                         # FastAPI backend
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chat.py                  # Chat and scoring endpoints
â”‚   â””â”€â”€ models.py                # Pydantic models
â”œâ”€â”€ gradio/                      # Gradio UI
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ui.py                    # Gradio interface
â”œâ”€â”€ settings.py                  # Configuration settings
â”œâ”€â”€ app_with_gradio.py          # Main FastAPI app with mounted Gradio
â””â”€â”€ Dockerfile                   # Updated Docker configuration
```

## ğŸš€ Running the Application

### Development
```bash
cd src
python app_with_gradio.py
```

### Docker
```bash
cd src
docker build -t x5-support .
docker run -p 8000:8000 x5-support
```

## ğŸ”§ API Endpoints

- `GET /health` - Health check
- `GET /api/health` - API health check
- `POST /api/chat` - Process chat messages
- `POST /api/set_score` - Handle user feedback
- `/` - Gradio UI (mounted at root)

## ğŸ“ Key Changes from Original

1. **Separation of Concerns**: AI logic, API, and UI are now in separate modules
2. **FastAPI Backend**: RESTful API endpoints for chat and feedback
3. **Gradio Frontend**: Clean UI that communicates with API via HTTP
4. **Better Structure**: Modular architecture for easier maintenance
5. **Docker Support**: Updated Dockerfile for the new structure

## ğŸ”„ Request Flow

1. User input in Gradio UI
2. HTTP request to FastAPI `/api/chat` endpoint
3. API calls AI workflow (`run_workflow_with_tracing`)
4. Workflow processes query using existing logic
5. Response returned to API and then to UI
6. User feedback sent to `/api/set_score` endpoint

## ğŸ§© Original Logic Preserved

All original workflow logic from `ui.py` has been preserved:
- AI workflow steps remain unchanged
- Langfuse integration maintained
- Chat history management
- Like/dislike feedback system
- Error handling patterns

## ğŸ”§ Configuration

The application uses the same `settings.py` file for configuration:
- Langfuse settings
- LLM settings
- Embeddings settings
- Qdrant settings

## ğŸ³ Docker Deployment

The updated Dockerfile:
- Exposes port 8000 (instead of 7860)
- Runs the FastAPI app with mounted Gradio
- Maintains the same build process with uv