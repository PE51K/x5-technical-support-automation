include:
  - path: 'docker-compose.langfuse.yaml'
    env_file: './env/.env.langfuse'

services:
  llm:
    image: vllm/vllm-openai:latest
    ports:
      - 8000:8000
    volumes:
      - ./data/llm/vllm/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    # command: --model Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24 --dtype half --max_model_len 15000 --api-key token-123
    # command: --model google/gemma-2-9b-it --api-key token-123 --dtype half
    # command: --model IlyaGusev/saiga_nemo_12b --dtype half --max_model_len 15000 --api-key token-123
    command: --model unsloth/gemma-3-1b-it --dtype half --api-key token-123 --gpu-memory-utilization 0.60 --max-model-len 4096
    # command: --model Qwen/Qwen3-0.6B --dtype half --api-key token-123 --gpu-memory-utilization 0.80 --max-model-len 8192

  embedder:
    image: vllm/vllm-openai:latest
    ports:
      - 8001:8000
    volumes:
      - ./data/embedder/vllm/.cache/huggingface:/root/.cache/huggingface
    environment:
      # fall back to the standard Python downloader
      - HF_HUB_ENABLE_HF_TRANSFER=0
      # # (optional but recommended)
      # - HF_HUB_DOWNLOAD_TIMEOUT=180    # avoid time-outs on slow links
      # - HF_TOKEN=${HF_TOKEN}           # if the model is private
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    command: --model elderberry17/USER-bge-m3-x5-sentence

  qdrant:
    image: qdrant/qdrant
    ports:
      - 6333:6333
      - 6334:6334
    volumes:
      - ./data/qdrant/storage:/qdrant/storage

  # ui:
  #   build: 
  #     context: .
  #   pull_policy: build
  #   ports:
  #     - 7860:7860
  #   env_file:
  #     - .env
  #   environment:
  #     - PROD=true
  